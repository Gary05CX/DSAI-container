{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd31ca3d-3b10-4359-8a39-2e5716f3b364",
   "metadata": {},
   "source": [
    "Keynote of this lab:\n",
    "<ol>\n",
    "    <li>Introduce Langchain</li>\n",
    "    <li>Introduce chain</li>\n",
    "    <li>Let them remember what we said!!</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13d26066-fe67-4133-bb1f-a2df30cb6bcd",
   "metadata": {},
   "source": [
    "used packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcdb1d8c-2fff-4831-8349-2bd3ef948b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#connect llm\n",
    "import ollama\n",
    "#from ollama import chat\n",
    "#from ollama import ChatResponse\n",
    "#from ollama import Client\n",
    "\n",
    "#langchain\n",
    "import langchain\n",
    "import langgraph\n",
    "#import OllamaLLM\n",
    "\n",
    "#MongoDB\n",
    "import pymongo as pymg\n",
    "import pprint as pt\n",
    "import pandas as pd\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ac158b6-8242-4ab0-80c1-82cab0b3b3ed",
   "metadata": {},
   "source": [
    "use LLM to search website for you"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b99b565c-2d6a-4d9f-a525-523894d18b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate as cpt\n",
    "from langchain_ollama.llms import OllamaLLM as ollm\n",
    "from googlesearch import search as gs\n",
    "\n",
    "template: str = \"\"\"\n",
    "Question: {question}\n",
    "Based on the Question, list some keywords for the search engine.\n",
    "Answer: only Keywords.\n",
    "\"\"\"\n",
    "\n",
    "prompt = cpt.from_template(template)\n",
    "\n",
    "llm = \"llama3.2:latest\"\n",
    "\n",
    "model = ollm(model = llm, base_url = \"http://modelserver:11434\")\n",
    "\n",
    "chain = prompt | model\n",
    "\n",
    "response = chain.invoke({\"question\": \"怎麼能抽淘博陀螺\"})\n",
    "\n",
    "result = gs(response, num_results = 10, unique = True)\n",
    "\n",
    "for item in result:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07effdd8-aada-43df-ba30-d1c709a8feaf",
   "metadata": {},
   "source": [
    "simplest way to let LLM remember your conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e40e1e2-3fa9-4494-b2da-9b3568a265a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate as cpt\n",
    "from langchain_ollama.llms import OllamaLLM as ollm\n",
    "from langchain_core.messages import AIMessage, HumanMessage, SystemMessage\n",
    "\n",
    "msg: list = []\n",
    "\n",
    "template: str = \"\"\"\n",
    "Question: {question}\n",
    "Answer: Written in Chinese.\n",
    "\"\"\"\n",
    "\n",
    "prompt = cpt.from_template(template)\n",
    "\n",
    "llm = \"qwen2.5:7b\"\n",
    "\n",
    "model = ollm(model = llm, base_url = \"http://modelserver:11434\")\n",
    "\n",
    "chain = prompt | model\n",
    "\n",
    "def communication(question: dict) -> None:\n",
    "    msg.append(HumanMessage(content = question[\"question\"]))\n",
    "    response = chain.invoke(msg)\n",
    "    msg.append(AIMessage(content = response))\n",
    "    print(response)\n",
    "\n",
    "communication({\"question\": \"Who is BLG Bin?\"})\n",
    "communication({\"question\": \"How the sun rist affect BLG Bin?\"})\n",
    "communication({\"question\": \"What I have asked\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d7e1673-0dc4-48ce-80d2-529e859169f8",
   "metadata": {},
   "source": [
    "Save Conversion to Database (simplest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6b07646-f8f5-45d1-9e0e-8de6ed89c301",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate as cpt\n",
    "from langchain_ollama.llms import OllamaLLM as ollm\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "import pymongo as pymg \n",
    "\n",
    "def chain_log():\n",
    "    message = \"\"\n",
    "    chat_log = collection.find()\n",
    "    chat_log = list(chat_log)\n",
    "    if len(chat_log) != 0:\n",
    "        for chat in chat_log:\n",
    "            message += f\"Human:{chat[\"user\"]}\\nAI: {chat[\"ai\"]}\\n\"\n",
    "    else:\n",
    "        message += \"AI: This is a new conersion\"\n",
    "    return message        \n",
    "\n",
    "def update_chain(last) -> None:\n",
    "    post = db.conversation\n",
    "    upload = post.insert_one(last)\n",
    "    upload.inserted_id\n",
    "\n",
    "def communication(question) -> None:\n",
    "    response = rag_chain.invoke(question)\n",
    "    last_conversation = {\n",
    "        \"user\": question,\n",
    "        \"ai\": response\n",
    "    }\n",
    "    update_chain(last_conversation)\n",
    "    print(response)\n",
    "\n",
    "log = pymg.MongoClient(\"mongodb://mongos\", 27017)\n",
    "db = log[\"llm\"]\n",
    "db.drop_collection(\"conversation\")\n",
    "collection = db[\"conversation\"]\n",
    "\n",
    "template: str = \"\"\"\n",
    "You are Benson, a Data Science Teacher. \n",
    "Here is the conversion {history}\n",
    "communicate with human in a normal tone.\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = cpt.from_template(template)\n",
    "\n",
    "llm = \"qwen2.5:7b\"\n",
    "\n",
    "model = ollm(model = llm, base_url = \"http://modelserver:11434\")\n",
    "\n",
    "rag_chain = (\n",
    "    {\n",
    "        \"history\": lambda x: chain_log(),\n",
    "        \"question\": lambda x: RunnablePassthrough()\n",
    "    }\n",
    "    | prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "question = [\n",
    "    \"Hello, I am Benson, nice to meet you.\",\n",
    "    \"I would like to know how the differences between K-means and K-nearest neighbours algorithms are.\",\n",
    "    \"Summarise my asked question\"\n",
    "]\n",
    "\n",
    "for q in question:\n",
    "    print(f\"Question: {q}\")\n",
    "    communication(q)\n",
    "    print(\"=\" * 87)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39f0af16-53d5-47fb-b304-526e726ddcc2",
   "metadata": {},
   "source": [
    "Embbedding\n",
    "<br>\n",
    "source: <a href=\"https://zh.moegirl.org.cn/zh-hk/%E5%B4%A9%E5%9D%8F%EF%BC%9A%E6%98%9F%E7%A9%B9%E9%93%81%E9%81%93\">game setting</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a900bed-8287-4e29-9c20-a583585b7683",
   "metadata": {},
   "outputs": [],
   "source": [
    "#path checking\n",
    "import os\n",
    "\n",
    "#document preprocessing\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "#basic\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_ollama.llms import OllamaLLM\n",
    "from langchain_core.messages import AIMessage, HumanMessage, SystemMessage\n",
    "\n",
    "#embedding\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "#from langchain.vectorstores import Chroma\n",
    "from langchain_community.document_loaders import Docx2txtLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "\n",
    "\n",
    "documents = []\n",
    "\n",
    "for file in os.listdir(\"/Code/TrainingDataSet\"):\n",
    "    doc_path = \"/Code/TrainingDataSet/\" + file\n",
    "    loader = Docx2txtLoader(doc_path)\n",
    "    documents.extend(loader.load())\n",
    "\n",
    "\n",
    "text_splitter = CharacterTextSplitter(chunk_size = 2500, chunk_overlap = 2000)\n",
    "doc_splits = text_splitter.split_documents(documments)\n",
    "\n",
    "\n",
    "\n",
    "template: str = \"\"\"\n",
    "假設你是崩鐵：星穹鐵道裡的一位虛構史學家，你會與一位界外之民進行溝通。\n",
    "請根據{context}中的資訊加以整理，再用不同的角度及有趣口風回答{question}。\n",
    "你的回答必須是獨白或者文章的方式呈現，不能使用點列方式。\n",
    "\"\"\"\n",
    "\n",
    "prompt = cpt.from_template(template)\n",
    "\n",
    "llm = \"qwen2.5:7b\"\n",
    "\n",
    "model = ollm(model = llm, base_url = \"http://modelserver:11434\", temperature = 0.1)\n",
    "\n",
    "emded_target = OllamaEmbeddings(\n",
    "    model = \"mxbai-embed-large:latest\",\n",
    "    base_url = \"http://modelserver:11434\"\n",
    ")\n",
    "\n",
    "vector_store = InMemoryVectorStore.from_documents(documents = doc_splits, embedding = emded_target, collection_name = \"HSR\")\n",
    "\n",
    "retriever = vector_store.as_retriever()\n",
    "\n",
    "rag_chain = (\n",
    "    {\n",
    "        \"context\": retriever,\n",
    "        \"question\": RunnablePassthrough()\n",
    "    }\n",
    "    | prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "question_bank = [\n",
    "    \"崩鐵啟動！\",\n",
    "    \"下個版本有什麼能新角色\"\n",
    "]\n",
    "\n",
    "for i in question_bank:\n",
    "    print(f\"Question: {i}\")\n",
    "    response = rag_chain.invoke(i)\n",
    "    print(f\"Question: {response}\")\n",
    "    print(\"=\" * 87)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
